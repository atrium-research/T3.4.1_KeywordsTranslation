{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import evaluation dataset and dependecies\n",
    "\n",
    "Records is a list of dictionaries. Each dictionary specifies the original language of the article, metadata such as title and abstract, and a list of keywords. Each keyword is a dictionary with specified: the label, the wikidata_url, and the match. This is present even if the keyword is not in the original language of the article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import parse_excel_file, compute_precision, compute_recall\n",
    "from src.clients import OpenAIClient, AnthropicClient, GroqClient, OpenAIWebSearchClient\n",
    "from src.pipelines import EntityExtractionPipeline\n",
    "import os\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = parse_excel_file(\"./data/Dset_Eval_KW_Alignment_Eval_def.xlsx\")\n",
    "records = total_records[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the client\n",
    "\n",
    "Run the load_dotenv() for load all the env variables and run the cell with your client and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'openai'\n",
    "client = OpenAIClient(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'openai_websearch'\n",
    "client = OpenAIWebSearchClient(api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"gpt-4o-search-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'anthropic'\n",
    "client = AnthropicClient(api_key=os.getenv(\"ANTHROPIC_API_KEY\"), model_name=\"claude-opus-4-20250514\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'groq'\n",
    "client = GroqClient(api_key=os.getenv(\"GROQ_API_KEY\"), model_name=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Pipeline Initialization\n",
    "\n",
    "This cell runs the entity extraction pipeline using the input `.xlsx` file. It processes each article in the dataset and does the following for every keyword:\n",
    "\n",
    "1. **Generate Entity Candidates**\n",
    "   For each keyword, it generates up to 10 possible matching entities from Wikidata (default value set by `NUM_NAMES`).\n",
    "\n",
    "2. **Select Relevant Entity**\n",
    "   From the candidates, the model selects the best matching entity (by default, only 1 is selected).\n",
    "\n",
    "3. **Track Progress and Save Results**\n",
    "   The results are saved in a `.json` file with the original article data enriched by the LLM annotations for each keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = EntityExtractionPipeline(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data/evaluation_output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_EVERY = 3\n",
    "MAX_WORKERS = 10\n",
    "base_output_dir = './data/evaluation_output'\n",
    "base_filename = f'eval_{model}_{run_id}'\n",
    "results_file = os.path.join(base_output_dir, f\"{base_filename}.json\")\n",
    "adjusted_file = os.path.join(base_output_dir, f\"{base_filename}_adjusted.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import copy\n",
    "\n",
    "file_lock = threading.Lock()\n",
    "\n",
    "def process_keyword(record_data):\n",
    "    record_idx, kw_idx, language, title_or, abstract_or, kw_label = record_data\n",
    "    \n",
    "    try:\n",
    "        llm_uris = pipeline.extract_entities(\n",
    "            language=language,\n",
    "            title=title_or,\n",
    "            abstract=abstract_or,\n",
    "            keywords=kw_label\n",
    "        )\n",
    "        return record_idx, kw_idx, llm_uris, None\n",
    "    except Exception as e:\n",
    "        print(f\"LLM URIs cannot be computed for record {record_idx}, keyword {kw_idx}: {e}\")\n",
    "        return record_idx, kw_idx, [], str(e)\n",
    "\n",
    "tasks_data = []\n",
    "for record_idx, record in enumerate(records):\n",
    "    for kw_idx, kw in enumerate(record['kws']):\n",
    "        task_data = (\n",
    "            record_idx,\n",
    "            kw_idx,\n",
    "            record['language'],\n",
    "            record['title_eng'],\n",
    "            record['abstract_eng'],\n",
    "            kw['label']\n",
    "        )\n",
    "        tasks_data.append(task_data)\n",
    "\n",
    "print(f\"Preparati {len(tasks_data)} task da processare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Processa in parallelo\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Sottometti tutti i task con dati giÃ  copiati\n",
    "    future_to_task = {\n",
    "        executor.submit(process_keyword, task_data): task_data[0:2]  # record_idx, kw_idx\n",
    "        for task_data in tasks_data\n",
    "    }\n",
    "    \n",
    "    processed_count = 0\n",
    "    with tqdm(total=len(tasks_data), desc=\"Processing keywords\") as pbar:\n",
    "        for future in as_completed(future_to_task):\n",
    "            record_idx, kw_idx, llm_uris, error = future.result()\n",
    "            \n",
    "            # Aggiorna i risultati\n",
    "            records[record_idx]['kws'][kw_idx]['llm_uris'] = llm_uris\n",
    "            \n",
    "            processed_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Salvataggio periodico thread-safe\n",
    "            if processed_count % SAVE_EVERY == 0:\n",
    "                with file_lock:\n",
    "                    # print(f\"Saving partial results at {processed_count} processed keywords\")\n",
    "                    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Salvataggio finale\n",
    "print(\"All keywords processed, saving final results.\")\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell of code contains some required adjustments to the evaluation output for the results analysis. These adjustments mainly involve string parsing. Running this cell is required to obtain the statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_records = []\n",
    "\n",
    "with open(results_file, 'r', encoding='utf-8') as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "\n",
    "for record in records:\n",
    "    new_record = record\n",
    "    for i, kw in enumerate(record['kws']):\n",
    "        if len(kw['llm_uris']) == 3 and kw['llm_uris'][1] == \"wikidata\":\n",
    "            new_record['kws'][i]['llm_uris'] = ['.'.join(kw['llm_uris'])]\n",
    "    new_records.append(new_record)\n",
    "\n",
    "with open(adjusted_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_records, f, ensure_ascii=False, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The following two cells can be used to obtain the statistics. As can be seen, the recall, precision, and the F1 scores are printed. They are printed for all the examples, for language (that is, we distinguish the results based on the original language of the article of the keyword), and by match type (where for match type we consider how match the Wikidata URL matches the actual keyword, this could be 'e' - exact match -, or 'r' - related match).\n",
    "\n",
    "As can be seen in the example below, the code prints a report with the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_evaluate = adjusted_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_to_evaluate, 'r', encoding='utf-8') as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "languages = set(record['language'] for record in records)\n",
    "\n",
    "scores_llm = {\n",
    "    \"Total\": {\n",
    "        \"recall\": {\"Sum\": 0, \"Size\": 0},\n",
    "        \"precision\": {\"Sum\": 0, \"Size\": 0},\n",
    "        \"f1\": {\"Sum\": 0, \"Size\": 0}\n",
    "    },\n",
    "    \"Per_match_type\": {\n",
    "        \"e\": {\n",
    "            \"recall\": {\"Sum\": 0, \"Size\": 0},\n",
    "            \"precision\": {\"Sum\": 0, \"Size\": 0},\n",
    "            \"f1\": {\"Sum\": 0, \"Size\": 0}\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"recall\": {\"Sum\": 0, \"Size\": 0},\n",
    "            \"precision\": {\"Sum\": 0, \"Size\": 0},\n",
    "            \"f1\": {\"Sum\": 0, \"Size\": 0}\n",
    "        }\n",
    "    },\n",
    "    \"Per_language\": {\n",
    "        language: {\n",
    "            \"recall\": {\"Sum\": 0, \"Size\": 0},\n",
    "            \"precision\": {\"Sum\": 0, \"Size\": 0},\n",
    "            \"f1\": {\"Sum\": 0, \"Size\": 0}\n",
    "        } for language in languages\n",
    "    }\n",
    "}\n",
    "\n",
    "for record in records:\n",
    "    for i, kw in enumerate(record['kws']):\n",
    "        if kw['match'] in (\"e\", \"r\"):\n",
    "            correct_uris = [\n",
    "                url.replace(\"https\", \"http\").replace(\"/wiki/\", \"/entity/\")\n",
    "                for url in kw['wikidata_url']\n",
    "            ]\n",
    "            retrieved_uris_llm = kw['llm_uris']\n",
    "            recall_llm = compute_recall(correct_uris, retrieved_uris_llm)\n",
    "            precision_llm = compute_precision(correct_uris, retrieved_uris_llm)\n",
    "            \n",
    "            # Calcolo del punteggio F1 con controllo per divisione per zero\n",
    "            if (precision_llm + recall_llm) > 0:\n",
    "                f1_llm = 2 * precision_llm * recall_llm / (precision_llm + recall_llm)\n",
    "            else:\n",
    "                f1_llm = 0\n",
    "\n",
    "            # Aggiornamento dei punteggi totali\n",
    "            scores_llm[\"Total\"][\"recall\"][\"Sum\"] += recall_llm\n",
    "            scores_llm[\"Total\"][\"recall\"][\"Size\"] += 1\n",
    "            scores_llm[\"Total\"][\"precision\"][\"Sum\"] += precision_llm\n",
    "            scores_llm[\"Total\"][\"precision\"][\"Size\"] += 1\n",
    "            scores_llm[\"Total\"][\"f1\"][\"Sum\"] += f1_llm\n",
    "            scores_llm[\"Total\"][\"f1\"][\"Size\"] += 1\n",
    "\n",
    "            # Aggiornamento dei punteggi per tipo di match\n",
    "            scores_llm[\"Per_match_type\"][kw['match']][\"recall\"][\"Sum\"] += recall_llm\n",
    "            scores_llm[\"Per_match_type\"][kw['match']][\"recall\"][\"Size\"] += 1\n",
    "            scores_llm[\"Per_match_type\"][kw['match']][\"precision\"][\"Sum\"] += precision_llm\n",
    "            scores_llm[\"Per_match_type\"][kw['match']][\"precision\"][\"Size\"] += 1\n",
    "            scores_llm[\"Per_match_type\"][kw['match']][\"f1\"][\"Sum\"] += f1_llm\n",
    "            scores_llm[\"Per_match_type\"][kw['match']][\"f1\"][\"Size\"] += 1\n",
    "\n",
    "            # Aggiornamento dei punteggi per lingua\n",
    "            scores_llm[\"Per_language\"][record['language']][\"recall\"][\"Sum\"] += recall_llm\n",
    "            scores_llm[\"Per_language\"][record['language']][\"recall\"][\"Size\"] += 1\n",
    "            scores_llm[\"Per_language\"][record['language']][\"precision\"][\"Sum\"] += precision_llm\n",
    "            scores_llm[\"Per_language\"][record['language']][\"precision\"][\"Size\"] += 1\n",
    "            scores_llm[\"Per_language\"][record['language']][\"f1\"][\"Sum\"] += f1_llm\n",
    "            scores_llm[\"Per_language\"][record['language']][\"f1\"][\"Size\"] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def compute_mean_metrics(stats_dict):\n",
    "    \"\"\"\n",
    "    Dato un dizionario con la struttura:\n",
    "    {\n",
    "      'Total': {\n",
    "        'recall': {'Sum': x, 'Size': y},\n",
    "        'precision': {'Sum': x2, 'Size': y2},\n",
    "        'f1': {'Sum': x3, 'Size': y3}\n",
    "      },\n",
    "      # 'Per_match_type': {\n",
    "        'e': {\n",
    "          'recall': {'Sum': x, 'Size': y},\n",
    "          'precision': {'Sum': x2, 'Size': y2},\n",
    "          'f1': {'Sum': x3, 'Size': y3}\n",
    "        },\n",
    "        ...\n",
    "      },\n",
    "      'Per_language': {\n",
    "        'fr': {\n",
    "          'recall': {'Sum': x, 'Size': y},\n",
    "          'precision': {'Sum': x2, 'Size': y2},\n",
    "          'f1': {'Sum': x3, 'Size': y3}\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "    }\n",
    "\n",
    "    Restituisce un dizionario con i valori medi di recall, precision e f1.\n",
    "    \"\"\"\n",
    "    mean_dict = {\n",
    "        'Total': {},\n",
    "        'Per_match_type': {},\n",
    "        'Per_language': {}\n",
    "    }\n",
    "    \n",
    "    # --- 1) TOTAL ---\n",
    "    if 'Total' in stats_dict:\n",
    "        total_recall_sum = stats_dict['Total']['recall']['Sum']\n",
    "        total_recall_size = stats_dict['Total']['recall']['Size']\n",
    "        total_precision_sum = stats_dict['Total']['precision']['Sum']\n",
    "        total_precision_size = stats_dict['Total']['precision']['Size']\n",
    "        \n",
    "        mean_dict['Total']['recall'] = total_recall_sum / total_recall_size if total_recall_size != 0 else 0\n",
    "        mean_dict['Total']['precision'] = total_precision_sum / total_precision_size if total_precision_size != 0 else 0\n",
    "        \n",
    "        if 'f1' in stats_dict['Total']:\n",
    "            total_f1_sum = stats_dict['Total']['f1']['Sum']\n",
    "            total_f1_size = stats_dict['Total']['f1']['Size']\n",
    "            mean_dict['Total']['f1'] = total_f1_sum / total_f1_size if total_f1_size != 0 else 0\n",
    "\n",
    "    # --- 2) PER MATCH TYPE ---\n",
    "    if 'Per_match_type' in stats_dict:\n",
    "        for match_type, metrics in stats_dict['Per_match_type'].items():\n",
    "            recall_sum = metrics['recall']['Sum']\n",
    "            recall_size = metrics['recall']['Size']\n",
    "            precision_sum = metrics['precision']['Sum']\n",
    "            precision_size = metrics['precision']['Size']\n",
    "            \n",
    "            mean_rec = recall_sum / recall_size if recall_size != 0 else 0\n",
    "            mean_prec = precision_sum / precision_size if precision_size != 0 else 0\n",
    "            \n",
    "            mean_dict['Per_match_type'][match_type] = {\n",
    "                'recall': mean_rec,\n",
    "                'precision': mean_prec\n",
    "            }\n",
    "            if 'f1' in metrics:\n",
    "                f1_sum = metrics['f1']['Sum']\n",
    "                f1_size = metrics['f1']['Size']\n",
    "                mean_dict['Per_match_type'][match_type]['f1'] = f1_sum / f1_size if f1_size != 0 else 0\n",
    "\n",
    "    # --- 3) PER LANGUAGE ---\n",
    "    if 'Per_language' in stats_dict:\n",
    "        for lang, metrics in stats_dict['Per_language'].items():\n",
    "            recall_sum = metrics['recall']['Sum']\n",
    "            recall_size = metrics['recall']['Size']\n",
    "            precision_sum = metrics['precision']['Sum']\n",
    "            precision_size = metrics['precision']['Size']\n",
    "            \n",
    "            mean_rec = recall_sum / recall_size if recall_size != 0 else 0\n",
    "            mean_prec = precision_sum / precision_size if precision_size != 0 else 0\n",
    "            \n",
    "            mean_dict['Per_language'][lang] = {\n",
    "                'recall': mean_rec,\n",
    "                'precision': mean_prec\n",
    "            }\n",
    "            if 'f1' in metrics:\n",
    "                f1_sum = metrics['f1']['Sum']\n",
    "                f1_size = metrics['f1']['Size']\n",
    "                mean_dict['Per_language'][lang]['f1'] = f1_sum / f1_size if f1_size != 0 else 0\n",
    "                \n",
    "    return mean_dict\n",
    "\n",
    "\n",
    "def print_system_report(system_dict):\n",
    "    \"\"\"\n",
    "    Stampa un report leggibile dei risultati del sistema (LLM) per Total,\n",
    "    Per_match_type e Per_language includendo F1.\n",
    "    \"\"\"\n",
    "    system_means = compute_mean_metrics(system_dict)\n",
    "    \n",
    "    print(\"======== SYSTEM METRICS REPORT ========\")\n",
    "    \n",
    "    # --- 1) TOTAL ---\n",
    "    print(\"\\n--- TOTAL ---\")\n",
    "    total = system_means.get('Total', {})\n",
    "    if total:\n",
    "        print(f\"LLM Recall:    {total.get('recall', 0):.4f}\")\n",
    "        print(f\"LLM Precision: {total.get('precision', 0):.4f}\")\n",
    "        print(f\"LLM F1:        {total.get('f1', 0):.4f}\")\n",
    "    else:\n",
    "        print(\"No total data found.\")\n",
    "    \n",
    "    # --- 2) PER MATCH TYPE ---\n",
    "    print(\"\\n--- PER MATCH TYPE ---\")\n",
    "    per_match_data = system_means.get('Per_match_type', {})\n",
    "    for mtype, vals in per_match_data.items():\n",
    "        print(f\"\\nMatch Type: {mtype}\")\n",
    "        print(f\"  LLM Recall:    {vals.get('recall', 0):.4f}\")\n",
    "        print(f\"  LLM Precision: {vals.get('precision', 0):.4f}\")\n",
    "        print(f\"  LLM F1:        {vals.get('f1', 0):.4f}\")\n",
    "    \n",
    "    # --- 3) PER LANGUAGE ---\n",
    "    print(\"\\n--- PER LANGUAGE ---\")\n",
    "    per_lang_data = system_means.get('Per_language', {})\n",
    "    # Ordinamento convertendo le chiavi in stringa per evitare errori se sono di tipo misto\n",
    "    for lang, vals in sorted(per_lang_data.items(), key=lambda x: str(x[0])):\n",
    "        print(f\"\\nLanguage: {lang}\")\n",
    "        print(f\"  LLM Recall:    {vals.get('recall', 0):.4f}\")\n",
    "        print(f\"  LLM Precision: {vals.get('precision', 0):.4f}\")\n",
    "        print(f\"  LLM F1:        {vals.get('f1', 0):.4f}\")\n",
    "\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "# Supponiamo che 'scores_llm' contenga i risultati del sistema (LLM)\n",
    "# Ad esempio, scores_llm Ã¨ stato popolato precedentemente nel codice.\n",
    "print_system_report(scores_llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
