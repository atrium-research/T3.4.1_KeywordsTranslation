{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tools_utils' from 'c:\\\\Users\\\\paolo\\\\Desktop\\\\gotriple-keyword-translation-main\\\\tools_utils.py'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import importlib\n",
    "import data_utils\n",
    "import tools_utils\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "\n",
    "#  to reload external modules\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(tools_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains functions to use tools to map GoTriple keywords to WikiData pages. Each function takes as input an item from the list produced by the function get_sample in data_utils.py (each item contains information about the title, the abstract and the keywords of the article, see data_utils.py for further details)\n",
    "\n",
    "- The first function uses DBPedia Spotlight. It maps keywords to DBPedia resources (each keyword is mapped to the DBPedia correspondent to the keywords language) if the language of the keyword is different than 'en' (English). \n",
    "    - The parameter 'context' specifies the text that is given as input to DBPedia Spotlight. If True, the title and abstract are given as additional context (this slows execution since DBPedia Spotlight annotates the whole input). If False, only keywords are given as input\n",
    "    - If keywords are in English, a further mapping from DBPedia to Wikidata resources is performed (using SPARQL). This feature is not available for other languages for what it seems like a lack of semantic annotation (the SPARQL engine is not available for DBPedia corresponding to some languages and there is shortage of annotated links between pages in different languages). Moreover, it should be noted that the performance of DBPedia Spotlight seems to be poorer when it is used in languages other than English (THIS NEEDS VERIFICATION).\n",
    "    - The function returns a list where each element corresponds to an annotation. Each element of the returned list has three keys: 'Form' (specifies the surface form that DBPedia Spotlight has linked to the URI), 'DBPediaURI' and 'WikiDataURI' (contains None if the language of the keywords is not English) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "example of calling the function:\n",
    "data = data_utils.get_sample(['en', 'fr', 'es'], 300)\n",
    "test_item = data[0]\n",
    "output = useDBPediaSpotlight(test_item, False)\n",
    "\"\"\"\n",
    "\n",
    "def useDBPediaSpotlight(item, context):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    #  useful to retrieve entities relevant to the keywords (and not to the context) in case of search with context\n",
    "    keywords_token = [token for kw in item['Keywords'] for token in kw.split(' ')]\n",
    "\n",
    "    abstract = item['Abstract_or'] if item['Abstract_or'] else \"\"\n",
    "    title = item['Title_or'] if item['Title_or'] else \"\"\n",
    "\n",
    "    #  prepare the text for the query\n",
    "    if context:\n",
    "        text = 'Title: ' + title + '. ' + 'Abstract: ' + abstract + '. Keywords: ' + \", \".join(item['Keywords'])\n",
    "    else:\n",
    "        text = \", \".join(item['Keywords'])\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    #  send a request to DBPedia Spotlight API\n",
    "    data = tools_utils.queryAPIDBpediaSpotlight(text, item['Language'])['Resources']\n",
    "\n",
    "    #  processes the output to retain only entities corresponding to keywords\n",
    "    for entity in data:\n",
    "        if context:\n",
    "            if entity['@surfaceForm'] in keywords_token:\n",
    "                results.append({'Form': entity['@surfaceForm'], 'DBPediaURI': entity['@URI']})\n",
    "        else: \n",
    "            results.append({'Form': entity['@surfaceForm'], 'DBPediaURI': entity['@URI']})\n",
    "\n",
    "    #  conversion of DBPedia URIs in Wikidata URIs\n",
    "    for result in results:\n",
    "        if item['Language'] == 'en':\n",
    "            result['WikidataURI'] = tools_utils.get_wikidata_uri(result['DBPediaURI'])\n",
    "        else: \n",
    "            result['WikidataURI'] = None\n",
    "\n",
    "    \n",
    "    #  remove duplicates\n",
    "    words_in_results = []\n",
    "    final_results = []\n",
    "    for result in results:\n",
    "        if result['Form'] not in words_in_results:\n",
    "            words_in_results.append(result['Form'])\n",
    "            final_results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second function uses LLMs. In this notebook, we use Llama.cpp, a library where various LLMs are implemented in C++ in order to allow faster inference times even on CPU. The library allows inference on variety of quantized LLMs available on HuggingFace in GGUF format. We use the library in order to allow replicability of the code without requiring specialized software. Specifically, we use llama-cpp-python, a Python wrapper of the library.\n",
    "    - The function takes as input item (an item from the list produced by the function get_sample in data_utils.py), model (a model loaded via the Python wrapper of the Llama.cpp library), and context (controls the context that we provide to the model: \"Title\" if we want to provide the title with no abstract, \"All\" if we want to provide the title and the abstract, otherwise we will provide only the keywords). Providing the abstract slows performance since the prompt is longer. A good compromise is to provide only the title in order to limit the length of the prompt and still provide context to the model\n",
    "    - The function prompts the model for the Wikidata entities corresponding to the keywords and then performs a query on Wikidata using the WikiData API (the requests and the method for searching the best fit are in a function in tools_utils.py). It returns a list of dictionaries where each dictionary has a field 'Keyword' and a field 'URI' (the second has no value if the query gives no result)\n",
    "\n",
    "NB: This feature is only for experimentation since it has very slow response time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "example of calling the function:\n",
    "data = data_utils.get_sample(['en', 'fr', 'es'], 300)\n",
    "test_item = data[0]\n",
    "llm = tools_utils.loadLLM() (load by default 4-bit quantization of Mistral-7B-Instruct)\n",
    "useLLM(data[35], llm, context=\"Title\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def useLLM(item, model, context):\n",
    "    if context == \"Title\":\n",
    "        prompt = \"\"\"<s>[INST] {{Map each keyword of the article to one or more relevant WikiData entities.\n",
    "        Keywords are from a scientific article. \n",
    "        The title of the article is {}.\n",
    "        The keyword list is: {}. \n",
    "        An example of answer for the article with the title \"Russian formalists and Russian literature\"\n",
    "        and the list of keywords: literary life, literary fact, doing things\n",
    "        is: literary life: [literature]; literary fact: [literature], [fact]; doing things: [activity]\n",
    "        INCLUDE EACH SEPARATE ENTITY BETWEEN [] IN THE ANSWER }} [/INST]\n",
    "    \"\"\".format(item['Title_or'], \", \".join([kw for kw in item['Keywords']]))   \n",
    "    if context == \"All\":\n",
    "        prompt = \"\"\"<s>[INST] {{Map each keyword of the article to one or more relevant WikiData entities.\n",
    "        Keywords are from a scientific article. \n",
    "        The title of the article is {}.\n",
    "        The abstract of the article is {}.\n",
    "        The keyword list is: {}. \n",
    "        An example of answer for the article with the title \"Russian formalists and Russian literature\"\n",
    "        and the list of keywords: literary life, literary fact, doing things\n",
    "        is: literary life: [literature]; literary fact: [literature], [fact]; doing things: [activity]\n",
    "        INCLUDE EACH SEPARATE ENTITY BETWEEN [] IN THE ANSWER }} [/INST]\n",
    "    \"\"\".format(item['Title_or'], item['Abstract_or'], \", \".join([kw for kw in item['Keywords']])) \n",
    "    else:\n",
    "        prompt = \"\"\"<s>[INST] {{Map each keyword to one or more relevant WikiData entities.\n",
    "        Keywords are from a scientific article. \n",
    "        The keyword list is: {}. \n",
    "        An example of answer for the list of keywords: literary life, literary fact, doing things\n",
    "        is: literary life: [literature]; literary fact: [literature], [fact]; doing things: [activity]\n",
    "        INCLUDE EACH SEPARATE ENTITY BETWEEN [] IN THE ANSWER }} [/INST]\n",
    "    \"\"\".format(\", \".join([kw for kw in item['Keywords']]))\n",
    "\n",
    "\n",
    "    output = model(\n",
    "      prompt,\n",
    "      max_tokens=200, \n",
    "    ) \n",
    "\n",
    "    #  formatting of the answer (this procedure is dependent on the output form we impose via the prompt.)\n",
    "    entities = re.findall(r'\\[([^\\]]+)\\]', output['choices'][0]['text'])\n",
    "\n",
    "    results = []\n",
    "    for entity in entities:\n",
    "        item = {}\n",
    "        item['Keyword'] = entity\n",
    "        print(entity.lower().split(\"()\")[0])\n",
    "        uri = tools_utils.query_wikidata(entity.lower().split(\"()\")[0])\n",
    "        if uri:\n",
    "            item['URI'] = uri['concepturi']\n",
    "        else:\n",
    "            item['URI'] = ''\n",
    "        results.append(item)\n",
    "\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
